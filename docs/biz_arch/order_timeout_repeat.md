# 订单超时和防重

## 超时订单处理

![超时订单_整体架构](./imgs/超时订单_整体架构.jpg)

### 被动超时处理

当用户发起订单状态查询或发起`确认支付`操作时，主动查询订单判断是否超时；如果超时，则将订单关闭并返回`订单已失效`，否则再继续后续流程

该方案是执行`确认订单`的入口逻辑，是所有方案的兜底方案。建议还是强依赖数据库，以保证订单的时效性

如果只依赖该方案处理超时订单，如果用户不再发起任何操作，那么订单的状态就不会再继续流转；针对小型系统：使用`被动超时处理` + `定时任务超时处理`足以应对

在大型系统中，可以基于`分库分表`提升数据库的操作性能；同时基于`缓存化`缓存订单信息，如果缓存中的订单已经是下一个状态，则不再校验超时（需要考虑缓存和数据库一致性问题：先删再改，改完再删，队列同步？）

### 定时任务超时处理

以一个较为快的频率启动一个定时任务，定时扫描订单表，条件是状态是`初始化` + `失效时间 < current_time`捞取记录；捞取记录后，基于乐观锁，更新订单到`关闭`状态。

在小型的系统中，个人认为该方案足以应对订单超时场景。只要做好并发控制，控制每次捞取的记录数避免系统出现`OOM`问题即可；

同时，我也任务该方案可以作为大型系统的兜底方案（因为我们不能保证延迟队列的高可用和投递成功率就是100%），但是需要考虑在大数据量情况下的性能问题

- 如果订单已经做了分库分表，那么定时任务也应该基于sharding的方式运行：这样可以保证每一个运行节点同时运行，同时降低系统压力；建议shard数和分库数一致，或者每个节点平分订单shard库的数据

- 控制任务频率和每批记录数，因为还有`被动超时处理`作为兜底，降低对数据库（即使是从库）的压力；可以在job捞数的过程中添加timer计时器，例如`每隔30s`运行的任务，捞数过程不超过`20s`，这样可以为更新预留出时间buffer，这样可以避免由于job时间过长，导致后续任务捞取重复记录

- 异步化：捞取超时记录后，可以压到本地线程池异步操作（要注意应用shutdown时，为pool增加hook）

### 延迟消息超时处理

#### 基于消息中间件特性实现延迟队列

- RabbitMQ死信队列模式：
> 发送消息时设置消息TTL，针对该topic不设置消费者，当消息超时后，会进入到死信队列
>
> 新建exchange绑定死信队列，并新增添加Consumer，消费后处理订单关闭
>
> 问题：如果只设置一个队列接受TTL消息，第一个消息10min后超时，第二个消息1min后超时，那么第二条消息也会在10min后被处理；
>
> 解决方案：根据RabbitMQ的架构特点，技术基于镜像模式做高可用，每个节点的queue数据也是一致的；我们要为不同的延时时间，建立多个Queue

- RocketMQ延迟队列模式：
> 发送消息时，设置消息的delayTimeLevel级别，消息推送到Broker之后，会暂存在`SCHEDULE_TOPIC_实际Topic名称`这个Topic中
>
> 开源版支持`1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h`，足够一般业务场景使用
>
> Broker创建一个Timer，来执行不同级别的延时任务，读取`SCHEDULE_TOPIC_`的消息，将到期的Message推送到实际的`Topic`中，一直读取到当前queue中某一个offset位置的消息还没超时（单个queue中的消息是有序的）
>
> 问题：`java.util.Timer`其实只起了一个线程，然后处理queue中的的定时任务，当队列中有大量延迟消息时，性能相对较差
>
> 解决方案：修改源码，使用时间轮调度，每一个延时级别单独起一个线程处理，但是要处理Broker重启时，时间轮指针位置与圈数的问题

#### 基于时间轮实现延迟队列


**参考：**
> 订单超时自动关闭的实现方案总结: https://www.extutorial.com/blog/1543096
>
> 有赞延迟队列设计: https://tech.youzan.com/queuing_delay/
>
> 揭秘：达达-京东到家订单派发的技术实战: https://www.sohu.com/a/251506542_168370
>
> RocketMQ 延迟消息的使用与分析: https://blog.csdn.net/silence1144/article/details/109902684

----

## 订单防重处理